{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to train and register many models. We will utilize the [ParallelRunStep](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-parallel-run-step) in a pipeline to parallelize the process of training the models to make the process more efficient. \n",
    "\n",
    "For this solution accelerator we are using the [OJ Sales Dataset](https://azure.microsoft.com/en-us/services/open-datasets/catalog/sample-oj-sales-simulated/) to train 11,973 individual models that predict sales for each store and brand of orange juice. For more information about the data refer to the [data preparation notebook](01_Data_Preparation.ipynb).\n",
    "\n",
    "There are two ways to train many models:\n",
    "\n",
    "- Using a custom training script\n",
    "- Using Automated ML\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "At this point, you should have already:\n",
    "\n",
    "1. Created your AML Workspace using the [00_Setup_AML_Workspace notebook](00_Setup_AML_Workspace.ipynb)\n",
    "2. Run [01_Data_Preparation.ipynb](01_Data_Preparation.ipynb) to create the dataset\n",
    "\n",
    "Please ensure you have the latest version of the Azure ML SDK and also install Pipeline Steps Package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-sdk azureml-pipeline-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are planning to train the models using Automated ML you should also have the latest version of the `automl` extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-sdk[automl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to workspace and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Connect to workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get datastore\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "print('SDK version: ' + azureml.core.VERSION, \n",
    "      'Workspace Name: ' + ws.name,\n",
    "      'Azure Region: ' + ws.location,\n",
    "      'Subscription ID: ' + ws.subscription_id,\n",
    "      'Resource Group: ' + ws.resource_group,\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Get the training Dataset\n",
    "\n",
    "Next, we get the training Dataset using the [Dataset.get_by_name()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--) method.\n",
    "\n",
    "This is the training dataset we created and registered in the [data preparation notebook](01_Data_Preparation.ipynb). If you chose to use only a subset of the files, the training dataset name will be `oj_data_small_train`. Otherwise, the name you'll have to use is `oj_data_train`.\n",
    "\n",
    "We recommend to **start with the small dataset** and make sure everything runs successfully, then scale up to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'oj_data_small_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "dataset = Dataset.get_by_name(ws, name=dataset_name)\n",
    "dataset_input = dataset.as_named_input(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Choose a compute target\n",
    "\n",
    "Currently ParallelRunConfig only supports AMLCompute as compute target. This is the compute cluster you created in the [setup notebook](00_Setup_AML_Workspace.ipynb#3.0-Create-compute-cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster_name = 'cpucluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "\n",
    "compute = AmlCompute(ws, cpu_cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Build the training pipeline\n",
    "\n",
    "Now that the workspace, dataset and compute are set up, we can put together a pipeline for training. \n",
    "\n",
    "In the following subsections you'll need to pick an option to run depending on your choice for training:\n",
    "- Custom script\n",
    "- Automated ML\n",
    "\n",
    "The model we use as an example in the **custom script** version is a simple, regression-based forecaster built on scikit-learn and pandas utilities. See the [custom training script](scripts/train.py) to see how the forecaster is constructed. This forecaster is intended for demonstration purposes, so it does not handle the large variety of special cases that one encounters in time-series modeling. For instance, the model here assumes that all time-series are comprised of regularly sampled observations on a contiguous interval with no missing values. The model does not include any handling of categorical variables. You can of course modify this script to include feature engineering and the machine learning model of your choice.\n",
    "\n",
    "For a more general-use forecaster that handles missing data, advanced featurization, and automatic model selection, you can train using **Automated ML** with the [AutoML Forecasting task](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast). See the [AutoML training script](scripts/train_automl.py) to see how the forecaster is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Configure environment for ParallelRunStep\n",
    "\n",
    "An [environment](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments) defines a collection of resources that we will need to run our pipelines. We configure a reproducible Python environment for our training script. That environment will be replicated in the compute cluster for training.\n",
    "\n",
    "#### [Option A] Environment for custom script\n",
    "\n",
    "This environment will include the [scikit-learn](https://scikit-learn.org/stable/index.html) python library for training with a custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "train_env = Environment(name='many_models_environment_customscript')\n",
    "train_conda_deps = CondaDependencies.create(\n",
    "    pip_packages=['sklearn', 'pandas', 'joblib', 'azureml-core', 'azureml-dataprep[fuse]'])\n",
    "train_env.python.conda_dependencies = train_conda_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Option B] Environment for AutoML\n",
    "\n",
    "Training with Automated ML requires a more complex environment. We will create it using the `get_automl_environment` helper function located under the scripts folder of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.notebooks.modeling import get_automl_environment\n",
    "train_env = get_automl_environment(name='many_models_environment_automl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Set up ParallelRunConfig\n",
    "\n",
    "[ParallelRunConfig](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallel_run_config.parallelrunconfig?view=azure-ml-py) provides the configuration for the ParallelRunStep we'll be creating next. Here we specify the environment and compute target we created above along with the entry script that will be for each batch.\n",
    "\n",
    "There's a number of important parameters to configure including:\n",
    "- **mini_batch_size**: The number of files per batch. If you have 500 files and mini_batch_size is 10, 50 batches would be created containing 10 files each. Batches are split across the various nodes. We'll set this to 1.\n",
    "\n",
    "- **node_count**: The number of compute nodes to be used for running the user script. For the small sample of OJ datasets, we only need a single node, but you will likely need to increase this number for larger datasets composed of more files. If you increase the node count beyond 20 here, you may need to increase the max_nodes for the compute cluster as well.\n",
    "\n",
    "- **process_count_per_node**: The number of processes per node. The compute cluster we are using has 8 cores, so that should be the limit.\n",
    "\n",
    "- **run_invocation_timeout**: The run() method invocation timeout in seconds. The timeout should be set to be higher than the maximum training time of one model (in seconds), by default it's 60.\n",
    "\n",
    "We have determined the settings that work best for the orange juice use case depending on the type of training we are using, but you can play with the parameters and see how this affect training time.\n",
    "\n",
    "[Option A] Configuration for custom script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = 1\n",
    "process_count_per_node = 8\n",
    "run_invocation_timeout = 180\n",
    "training_script = 'train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Option B] Configuration for AutoML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = 2\n",
    "process_count_per_node = 6\n",
    "run_invocation_timeout = 3700\n",
    "training_script = 'train_automl.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the `ParallelRunConfig` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script=training_script,\n",
    "    mini_batch_size=\"1\",\n",
    "    run_invocation_timeout=run_invocation_timeout,\n",
    "    error_threshold=-1,\n",
    "    output_action=\"append_row\",\n",
    "    environment=train_env,\n",
    "    process_count_per_node=process_count_per_node,\n",
    "    compute_target=compute,\n",
    "    node_count=node_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And validate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.notebooks.modeling import validate_parallel_run_config\n",
    "\n",
    "validate_parallel_run_config(parallel_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  *[Optional]* Define AutoML settings\n",
    "\n",
    "You'll need to run this section if you are planning to train the models using Automated ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary defines the AutoML settings, for this forecasting task we add the name of the time column and the maximum forecast horizon.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n",
    "|**blacklist_models**|Models in blacklist won't be used by AutoML. All supported models can be found at [here](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.forecasting?view=azure-ml-py).|\n",
    "|**iterations**|Number of models to train. This is optional but provides customer with greater control.|\n",
    "|**iteration_timeout_minutes**|Maximum amount of time in minutes that the model can train. This is optional and depends on the dataset. We ask customer to explore a bit to get approximate times for training the dataset. For OJ dataset we set it 20 minutes|\n",
    "|**experiment_timeout_hours**|Maximum amount of time in hours that the experiment can take before it terminates.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**n_cross_validations**|Number of cross validation splits. Rolling Origin Validation is used to split time-series in a temporally consistent way.|\n",
    "|**enable_early_stopping**|Flag to enable early termination if the score is not improving in the short term.|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**max_horizon**|The number of periods out you would like to predict past your training data. Periods are inferred from your data.|\n",
    "|**grain_column_names**|The column names used to uniquely identify timeseries in data that has multiple rows with the same timestamp.|\n",
    "|**group_column_names**|The names of columns used to group your models. For timeseries, the groups must not split up individual time-series. That is, each group must contain one or more whole time-series.|\n",
    "|**track_child_runs**|Flag to disable tracking of child runs. Only best run (metrics and model) is tracked if the flag is set to False.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from scripts.notebooks.modeling import write_automl_settings_to_file\n",
    "\n",
    "automl_settings = {\n",
    "    'task': 'forecasting',\n",
    "    'primary_metric': 'normalized_root_mean_squared_error',\n",
    "    'iteration_timeout_minutes': 10, # This needs to be changed based on the dataset. We ask customer to explore how long training is taking before settings this value\n",
    "    'iterations': 15,\n",
    "    'experiment_timeout_hours': 1,\n",
    "    'label_column_name': 'Quantity',\n",
    "    'n_cross_validations' : 3,\n",
    "    'verbosity': logging.INFO, \n",
    "    'debug_log': 'automl_oj_sales_debug.txt',\n",
    "    'time_column_name': 'WeekStarting',\n",
    "    'max_horizon': 6,\n",
    "    'group_column_names': ['Store', 'Brand'],\n",
    "    'grain_column_names': ['Store', 'Brand']\n",
    "}\n",
    "\n",
    "write_automl_settings_to_file(automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Set up ParallelRunStep\n",
    "\n",
    "This [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallelrunstep?view=azure-ml-py) is the main step in our training pipeline. \n",
    "\n",
    "First, we set up the output directory and define the pipeline's output name. The datastore that stores the pipeline's output data is Workspace's default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "output_dir = PipelineData(name='training_output', datastore=dstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide our ParallelRunStep with a name, the ParallelRunConfig created above and several other parameters:\n",
    "\n",
    "- **inputs**: A list of input datasets. Here we'll use the dataset created in the data preparation notebook and retrieved in step 2. The number of files in the FileDataset determines the number of models that will be trained in the ParallelRunStep.\n",
    "\n",
    "- **output**: A PipelineData object that corresponds to the output directory. We'll use the output directory we just defined. \n",
    "\n",
    "- **allow_reuse**: Indicates whether the step should reuse previous results when re-run with the same settings. \n",
    "\n",
    "- **arguments**: A list of arguments required for the train.py entry script. Here, we provide the schema for the timeseries data - i.e. the names of target, timestamp, and id columns - as well as columns that should be dropped prior to modeling and a string identifying the model type.\n",
    "\n",
    "\n",
    "[Option A] Parameters for custom script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_arguments = [\n",
    "    '--target_column', 'Quantity', \n",
    "    '--timestamp_column', 'WeekStarting', \n",
    "    '--timeseries_id_columns', 'Store', 'Brand',\n",
    "    '--drop_columns', 'Revenue', 'Store', 'Brand',\n",
    "    '--model_type', 'lr'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Option B] Parameters for Automated ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_arguments = [\n",
    "    '--drop_columns', 'Revenue',\n",
    "  # '--retrain_failed_models', 'True'  # Uncomment this if you want to retrain only failed models\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the ParallelRunStep object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "\n",
    "parallel_run_step = ParallelRunStep(\n",
    "    name='many-models-parallel-training',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[dataset_input],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False,\n",
    "    arguments=step_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Create pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will create the training pipeline, solely composed of the ParallelRunStep we have just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallel_run_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Run the training pipeline\n",
    "\n",
    "Next, we submit our pipeline to run. The run will train models for each dataset and compute **in-sample** accuracy metrics for the fits. \n",
    "\n",
    "Training time will depend on the method chosen and the number of files in the dataset. With our current ParallelRunConfig settings and using a Standard_D13_V2 VM:\n",
    "- Custom script: should take around 5-10 minutes with 10 files. With the full dataset it can take over an hour.\n",
    "- Automated ML: the whole training pipeline takes 20-30 minutes with 10 files, as it tries with many different algorithms and data transformation options and picks the one that yields better results. The full dataset takes hours to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'many-models-training')\n",
    "print('Experiment name: ' + experiment.name)\n",
    "\n",
    "run = experiment.submit(pipeline)\n",
    "print('Run ID:', run.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the folowing command if you'd like to monitor the training process in jupyter notebook. It will stream logs live while training. This command may not work for Notebook VM, however it should work on your local laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True, raise_on_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Monitor the training pipeline\n",
    "\n",
    "The run submitted to the Azure Machine Learning Training Compute Cluster may take a while to complete. You can monitor the status of the run in the [Azure Machine Learning Portal](https://ml.azure.com). When finished, the training pipeline will train and register models to the Workspace. Results can also be reviewed in the AML Portal.\n",
    "\n",
    "If there are any issues with training, you can inspect the run and explore logs under 'Outputs+logs'. You can look at the stdout and stderr output under logs/user/worker/*< worker ip >* for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_run = run.find_step_run('many-models-parallel-training')[0]\n",
    "print('URL to run:', step_run.get_portal_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can review registered models under the 'Models' section in the AML Portal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_id = ws.get_details()['id']\n",
    "models_url = f'https://ml.azure.com/model/list?wsid={workspace_id}'\n",
    "print('URL to models list:', models_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to cancel the runs of the training experiment you can use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for run in experiment.get_runs():\n",
    "#     if run.status == 'Running':\n",
    "#         print('Canceling run:', run.id)\n",
    "#         try:\n",
    "#             run.cancel() \n",
    "#         except Exception as e:\n",
    "#             print('Canceling run failed due to', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Analyze results of training pipeline\n",
    "\n",
    "The dataframe we return in the run method of the training script ([train.py](scripts/train.py) or [train_automl.py](scripts/train_automl.py)) is outputted to *parallel_run_step.txt*. To analyze the results of our training pipeline, we'll download that file, read in the data to a DataFrame, and then visualize the results, including the in-sample metrics. The output is not generated until the run is complete.\n",
    "\n",
    "### 7.1 Download parallel_run_step.txt locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this notebook has been restarted since you ran training, you might need to run the following to get the `run` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core import Experiment\n",
    "# from azureml.pipeline.core import PipelineRun\n",
    "\n",
    "# experiment = Experiment(ws, 'many-models-training')\n",
    "# run = PipelineRun(experiment, '<your-run-id>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then download the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_results(run, target_dir=None, step_name='many-models-parallel-training', output_name='training_output'):\n",
    "    stitch_run = run.find_step_run(step_name)[0]\n",
    "    port_data = stitch_run.get_output_data(output_name)\n",
    "    port_data.download(target_dir, show_progress=True)\n",
    "    return os.path.join(target_dir, 'azureml', stitch_run.id, output_name)\n",
    "\n",
    "file_path = download_results(run, 'output')\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Convert the file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path + '/parallel_run_step.txt', \n",
    "                 sep=' ',\n",
    "                 names=['Store', 'Brand', 'ModelType', 'FileName', 'ModelName', 'StartTime', 'EndTime',\n",
    "                        'RMSE', 'MAE', 'MAPE', 'Index', 'Number of Models', 'Status', 'ErrorType', 'ErrorMessage'], \n",
    "                 parse_dates=['StartTime', 'EndTime'])\n",
    "\n",
    "df['MSE'] = df['RMSE'] ** 2\n",
    "df['Duration'] = df['EndTime'] - df['StartTime']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df['EndTime'].max() - df['StartTime'].min()\n",
    "\n",
    "print('Number of Models: ' + str(len(df)))\n",
    "print('Total Duration: ' + str(total)[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average MAPE: ' + str(round(df['MAPE'].mean(), 5)))\n",
    "print('Average MSE: ' + str(round(df['MSE'].mean(), 5)))\n",
    "print('Average RMSE: ' + str(round(df['RMSE'].mean(), 5)))\n",
    "print('Average MAE: '+ str(round(df['MAE'].mean(), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum Duration: '+ str(df['Duration'].max())[7:])\n",
    "print('Minimum Duration: ' + str(df['Duration'].min())[7:])\n",
    "print('Average Duration: ' + str(df['Duration'].mean())[7:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Visualize Performance across models\n",
    "\n",
    "Here, we produce some charts from the errors metrics calculated during the run. It is important to note that these metrics are computed over the training data - that is, they're in-sample metrics - and, therefore, may not reflect true forecasting accuracy. Please see the [forecasting notebook](03_Forecasting_Pipeline.ipynb) for an example of out-of-sample evaluation that is more appropriate for assessing forecast accuracy.\n",
    "\n",
    "First, we examine the distribution of mean absolute percentage error (MAPE) over all the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = sns.boxplot(y='MAPE', data=df)\n",
    "fig.set_title('MAPE across all models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can break that down by Brand or Store to see variations in error across our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = sns.boxplot(x='Brand', y='MAPE', data=df)\n",
    "fig.set_title('MAPE by Brand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how long models for different brands took to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brand = df.groupby('Brand')\n",
    "brand = brand['Duration'].sum()\n",
    "brand = pd.DataFrame(brand)\n",
    "brand['time_in_seconds'] = [time.total_seconds()  for time in brand['Duration']]\n",
    "\n",
    "brand.drop(columns=['Duration']).plot(kind='bar')\n",
    "plt.xlabel('Brand')\n",
    "plt.ylabel('Seconds')\n",
    "plt.title('Total Training Time by Brand')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 *[Optional]* Publish and schedule the pipeline \n",
    "\n",
    "\n",
    "### 8.1 Publish the pipeline\n",
    "Once you have a pipeline you're happy with, you can publish a pipeline so you can call it programatically later on. See this [tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-your-first-pipeline#publish-a-pipeline) for additional information on publishing and calling pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_pipeline = pipeline.publish(\n",
    "#     name='many-models-training',\n",
    "#     description='Many Models training pipeline',\n",
    "#     version='1',\n",
    "#     continue_on_step_failure=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Schedule the pipeline\n",
    "You can also [schedule the pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-schedule-pipelines) to run on a time-based or change-based schedule. This could be used to automatically retrain models every month or based on another trigger such as data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "    \n",
    "# training_pipeline_id = published_pipeline.id\n",
    "\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Month\", interval=1, start_time=\"2020-01-01T09:00:00\")\n",
    "# recurring_schedule = Schedule.create(\n",
    "#     ws, \n",
    "#     name=\"training_pipeline_recurring_schedule\", \n",
    "#     description=\"Schedule Training Pipeline to run on the first day of every month\",\n",
    "#     pipeline_id=training_pipeline_id, \n",
    "#     experiment_name=experiment.name, \n",
    "#     recurrence=recurrence\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've trained and scored the models, move on to [03_Forecasting_Pipeline.ipynb](03_Forecasting_Pipeline.ipynb) to make forecasts with your models."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
